/*                       *
 * DO NOT EDIT THIS FILE *
 *                       */

/**************************************************************************//**
 *
 * @file authoritative_default.c
 *
 * @author Christopher A. Bohn
 *
 * @brief Functions to get authoritative results authoritative results.
 *
 ******************************************************************************/

/*
 * IntegerLab (c) 2018-26 Christopher A. Bohn
 *
 * Starter code licensed under the Apache License, Version 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 */

#include "authoritative_results.h"

void evaluate_addition(uint16_t operand1, uint16_t operand2, struct authoritative_result *result) {
    #if defined(__aarch64__)
        uint32_t sum;
        __asm__ volatile (
            "lsl %w5, %w5, #16\n\t"
            "lsl %w6, %w6, #16\n\t"
            "adds %w7, %w5, %w6\n\t"
            "cset %w1, eq\n\t"
            "cset %w2, mi\n\t"
            "cset %w3, vs\n\t"
            "cset %w4, cs\n\t"
            "lsr %w0, %w7, #16"
            :   "=&r"(result->result),
                "=&r"(result->zero_flag),
                "=&r"(result->sign_flag),
                "=&r"(result->overflow_flag),
                "=&r"(result->carry_flag),
                "+r"(operand1),
                "+r"(operand2),
                "=&r"(sum)
            :
            :   "cc"
            );
    #elif defined(__x86_64__)
        __asm__ volatile (
            "addw %5, %4\n\t"
            "setz %0\n\t"
            "sets %1\n\t"
            "seto %2\n\t"
            "setc %3\n\t"
            "movw %4, (%6)"
            :   "=&r"(result->zero_flag),
                "=&r"(result->sign_flag),
                "=&r"(result->overflow_flag),
                "=&r"(result->carry_flag),
                "+r"(operand1)
            :   "r"(operand2),
                "r"(result)
            :   "cc", "memory"
        );
    #else
        #warning Some of the *expected* flags from addition cannot be obtained for your system.
        result->result = operand1 + operand2;
        result->zero_flag = !result->result;
        result->sign_flag = ((int16_t)(result->result) < 0);
    #endif
}

void evaluate_subtraction(uint16_t operand1, uint16_t operand2, struct authoritative_result *result) {
    #if defined(__aarch64__)
        uint32_t difference;
        __asm__ volatile (
            "lsl %w5, %w5, #16\n\t"
            "lsl %w6, %w6, #16\n\t"
            "subs %w7, %w5, %w6\n\t"
            "cset %w1, eq\n\t"
            "cset %w2, mi\n\t"
            "cset %w3, vs\n\t"
            // "cset %w4, cs\n\t"    // it seems that Arm is using
            "cset %w4, cc\n\t"       // "borrow" instead of "carry"
            "lsr %w0, %w7, #16"
            :   "=&r"(result->result),
                "=&r"(result->zero_flag),
                "=&r"(result->sign_flag),
                "=&r"(result->overflow_flag),
                "=&r"(result->carry_flag),
                "+r"(operand1),
                "+r"(operand2),
                "=&r"(difference)
            :
            :   "cc"
        );
    #elif defined(__x86_64__)
        __asm__ volatile (
            "subw %5, %4\n\t"
            "setz %0\n\t"
            "sets %1\n\t"
            "seto %2\n\t"
            "setc %3\n\t"
            "movw %4, (%6)"
            :   "=&r"(result->zero_flag),
                "=&r"(result->sign_flag),
                "=&r"(result->overflow_flag),
                "=&r"(result->carry_flag),
                "+r"(operand1)
            :   "r"(operand2),
                "r"(result)
            :   "cc", "memory"
        );
    #else
        #warning Some of the *expected* flags from subtraction cannot be obtained for your system.
        result->result = operand1 - operand2;
        result->zero_flag = !result->result;
        result->sign_flag = ((int16_t)(result->result) < 0);
    #endif
}

void evaluate_unsigned_multiplication(uint16_t operand1, uint16_t operand2, struct authoritative_result *result) {
    #if defined(__aarch64__)
        uint32_t full_product;
        __asm__ volatile (
            "mul %w0, %w1, %w2\n\t"
            "strh %w0, [%3]\n\t"
            "lsr %w0, %w0, #16\n\t"
            "strh %w0, [%3, 2]"
            :   "=r"(full_product)
            :   "r"(operand1), "r"(operand2), "r"(result)
            :   "memory"
        );
    #elif defined(__x86_64__)
        uint16_t low_word, high_word;
        __asm__ volatile (
            "mulw %3"                       // %dx:%ax = %ax * operand
            :   "=a"(low_word), "=d"(high_word)
            :   "a"(operand1), "r"(operand2)
            :   "cc"
        );
        result->result = low_word;
        result->supplemental_result = high_word;
    #else
        #warning The *expected* supplemental result for multiplication cannot be obtained for your system.
        result->result = operand1 * operand2;
    #endif
}

void evaluate_unsigned_division(uint16_t operand1, uint16_t operand2, struct authoritative_result *result) {
    #if defined(__aarch64__)
        __asm__ volatile (
            "udiv %w0, %w2, %w3\n\t"
            "msub %w1, %w0, %w3, %w2"
            :   "=&r"(result->result),
                "=&r"(result->supplemental_result)
            :   "r"(operand1),
                "r"(operand2)
            :
        );
    #elif defined(__x86_64__)
        uint16_t low_word, high_word;
        __asm__ volatile (
            "xorw %1, %1\n\t"               // zero-extend %ax into %dx (sort of)
            "divw %3"                       // %ax = %dx:%ax / operand ; %dx = %dx:%ax % operand
            :   "=a"(low_word), "=&d"(high_word)
            :   "a"(operand1), "r"(operand2)
            :   "cc"
        );
        result->result = low_word;
        result->supplemental_result = high_word;
    #else
        result->result = operand1 / operand2;
        result->supplemental_result = operand1 % operand2;
    #endif
}

void evaluate_signed_multiplication(uint16_t operand1, uint16_t operand2, struct authoritative_result *result) {
    #if defined(__aarch64__)
        uint32_t full_product, extended_operand1, extended_operand2;
        __asm__ volatile (
            "sxth %w1, %w3\n\t"
            "sxth %w2, %w4\n\t"
            "mul %w0, %w1, %w2\n\t"
            "strh %w0, [%5]\n\t"
            "lsr %w0, %w0, #16\n\t"
            "strh %w0, [%5, 2]"
            :   "=&r"(full_product),
                "=&r"(extended_operand1),
                "=&r"(extended_operand2)
            :   "r"(operand1),
                "r"(operand2),
                "r"(result)
            : "memory"
        );
    #elif defined(__x86_64__)
        uint16_t low_word, high_word;
        __asm__ volatile (
            "imulw %3"                      // %dx:%ax = %ax * operand
            :   "=a"(low_word), "=d"(high_word)
            :   "a"(operand1), "r"(operand2)
            :   "cc"
        );
        result->result = low_word;
        result->supplemental_result = high_word;
    #else
        #warning The *expected* supplemental result for multiplication cannot be obtained for your system.
        result->result = (uint16_t) ((int16_t) operand1 * (int16_t) operand2);
    #endif
}

void evaluate_signed_division(uint16_t operand1, uint16_t operand2, struct authoritative_result *result) {
    #if defined(__aarch64__)
        uint32_t extended_operand1, extended_operand2;
        __asm__ volatile (
            "sxth %w2, %w4\n\t"
            "sxth %w3, %w5\n\t"
            "sdiv %w0, %w2, %w3\n\t"
            "msub %w1, %w0, %w3, %w2"
            :   "=&r"(result->result),
                "=&r"(result->supplemental_result),
                "=&r"(extended_operand1),
                "=&r"(extended_operand2)
            :   "r"(operand1),
                "r"(operand2)
            :
        );
    #elif defined(__x86_64__)
        uint16_t low_word, high_word;
        __asm__ volatile (
            "cwtd\n\t"
            "idivw %3"
            :   "=a"(low_word), "=&d"(high_word)
            :   "a"(operand1), "r"(operand2)
            :   "cc"
        );
        result->result = low_word;
        result->supplemental_result = high_word;
    #else
        result->result = (uint16_t) ((int16_t) operand1 / (int16_t) operand2);
        result->supplemental_result = (uint16_t) ((int16_t) operand1 % (int16_t) operand2);
    #endif
}
